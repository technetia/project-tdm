#! /usr/bin/env python

import string

class TokenizeError(Exception):
    pass


class Token(object):
    """
    Class representing a token generated by the lexer.
    """
    def __init__(self, token_type, lexeme):
        self.type = token_type
        self.lexeme = lexeme

    def __repr__(self):
        return "Token('{}', '{}')".format(self.type, self.lexeme)

    def __eq__(self, other):
        return self.type == other.type and self.lexeme == other.lexeme

    def __ne__(self, other):
        return self.type != other.type or self.lexeme != other.lexeme
    

# states of the finite state machine
STATES = (
    # default; invalid
    "NULL",
    # starting state
    "START",
    # dollar sign, beginning of 1/2-byte number
    "DOLLAR",
    # first byte digit 1
    "DIGIT1",
    # first byte digit 2
    "DIGIT2",
    # second byte digit 1
    "DIGIT3",
    # second byte digit 2
    "DIGIT4",
    # pound sign, beginning of 1-byte constant
    "POUND",
    # dollar sign that follows pound sign
    "PDOLLAR",
    # digit 1 of constant
    "PDIGIT1",
    # digit 2 of constant
    "PDIGIT2",
    # identifier
    "IDENT",
    # label
    "LABEL",
    # comma
    "COMMA",
    # left parenthesis
    "LPAREN",
    # right parenthesis
    "RPAREN",
    # comment (;)
    "COMMENT",
    # whitespace
    "WHITESPACE",
)

TOKEN_TYPES = dict(
    # identifier
    IDENT="IDENT",
    # 1-byte number
    INT="INT",
    # 2-byte number
    LONGINT="LONGINT",
    # immediate constant
    CONST="CONST",
    # register
    REGISTER="REGISTER",
    # comma
    COMMA="COMMA",
    # left parenthesis
    LPAREN="LPAREN",
    # right parenthesis
    RPAREN="RPAREN",
    # label
    LABEL="LABEL",
    # whitespace
    WHITESPACE="WHITESPACE",
    # invalid
    INVALID="INVALID"
)

STATE_TYPES = dict(
    NULL=TOKEN_TYPES["INVALID"],
    START=TOKEN_TYPES["INVALID"],
    DOLLAR=TOKEN_TYPES["INVALID"],
    DIGIT1=TOKEN_TYPES["INVALID"],
    DIGIT2=TOKEN_TYPES["INT"],
    DIGIT3=TOKEN_TYPES["INVALID"],
    DIGIT4=TOKEN_TYPES["LONGINT"],
    POUND=TOKEN_TYPES["INVALID"],
    PDOLLAR=TOKEN_TYPES["INVALID"],
    PDIGIT1=TOKEN_TYPES["INVALID"],
    PDIGIT2=TOKEN_TYPES["CONST"],
    IDENT=TOKEN_TYPES["IDENT"],
    LABEL=TOKEN_TYPES["LABEL"],
    COMMA=TOKEN_TYPES["COMMA"],
    LPAREN=TOKEN_TYPES["LPAREN"],
    RPAREN=TOKEN_TYPES["RPAREN"],
    COMMENT=TOKEN_TYPES["WHITESPACE"],
    WHITESPACE=TOKEN_TYPES["WHITESPACE"]
)

def _init_transitions():
    """
    Initializes the transitions of every state.

    Every state is initially given an entry of 256 NULLs.
    The state transitions are then manually coded in.

    The way this works is:
    Each of the 256 NULLs corresponds to a single character, through use of ord().
    When a character is read from a line of assembly code,
    the character's ord() value is used as a lookup for the next state.
    """
    transitions = {}
    for s in STATES:
        transitions[s] = ["NULL"] * 256

    def set_transition(state1, chars, state2):
        for c in chars:
            transitions[state1][ord(c)] = state2

    set_transition("START", string.whitespace, "WHITESPACE")
    set_transition("WHITESPACE", string.whitespace, "WHITESPACE")
    set_transition("START", string.ascii_letters, "IDENT")
    set_transition("IDENT", string.ascii_letters+string.digits, "IDENT")
    set_transition("IDENT", ":", "LABEL")
    set_transition("START", "$", "DOLLAR")
    set_transition("DOLLAR", string.digits, "DIGIT1")
    set_transition("DIGIT1", string.digits, "DIGIT2")
    set_transition("DIGIT2", string.digits, "DIGIT3")
    set_transition("DIGIT3", string.digits, "DIGIT4")
    set_transition("START", "#", "POUND")
    set_transition("POUND", "$", "PDOLLAR")
    set_transition("PDOLLAR", string.digits, "PDIGIT1")
    set_transition("PDIGIT1", string.digits, "PDIGIT2")
    set_transition("START", ",", "COMMA")
    set_transition("START", "(", "LPAREN")
    set_transition("START", ")", "RPAREN")
    set_transition("START", ";", "COMMENT")

    # comments consume everything after they begin
    transitions["COMMENT"] = ["COMMENT"] * 256
    return transitions
    
TRANSITIONS = _init_transitions()

def scan(line):
    """
    Parses a single line of assembly code and returns the tokenized version.
    """
    # list of tokens in line
    tokens = []
    # current index in line processing
    line_idx = 0
    # starting index of next token
    token_idx = 0
    # current transition state
    curr_state = "START"

    # if line is blank, immediately return
    # (parsing normally would incorrectly give a lexing error)
    if not line:
        return tokens

    while True:
        # obtain the next state to transition to
        next_state = "NULL"
        if line_idx < len(line):
            next_char = ord(line[line_idx])
            next_state = TRANSITIONS[curr_state][next_char]

        # if the next state to transition to is null
        if next_state == "NULL":

            # if ending at current state would give an invalid token
            if STATE_TYPES[curr_state] == TOKEN_TYPES["INVALID"]:
                # then the line of code is lexically invalid
                raise TokenizeError(
                    "Error in lexing {} at position {}, with token {}".format(
                        line, line_idx, line[token_idx:line_idx]))

            # if ending at current state would not give whitespace
            if STATE_TYPES[curr_state] != TOKEN_TYPES["WHITESPACE"]:
                # then we've formulated a complete token
                t = Token(STATE_TYPES[curr_state], line[token_idx:line_idx])
                tokens.append(t)

            # mark the start of next potential token
            token_idx = line_idx
            # reset state transition sequence
            curr_state = "START"
            # break if all characters read
            if line_idx >= len(line):
                break

        else:
            # next state to transition to not null
            # then save it and advance to next char
            curr_state = next_state
            line_idx += 1

    return tokens

